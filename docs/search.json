[
  {
    "objectID": "tools-reproducibility.html",
    "href": "tools-reproducibility.html",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "Tools course - R Session 1\nIn this part of Session 1, we will discuss the importance of reproducibility in statistical programming, and tools available in R to promote reproducible research."
  },
  {
    "objectID": "tools-introduction.html",
    "href": "tools-introduction.html",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "In the first part of Session 1, we will introduce the tools used in this course. We will primarily be working in Posit Workbench, but it is important to know how to install and use the tools on your own as well for future use.\n\n\n\n\n\n\nLearning to program\n\n\n\nIf this is your first time learning to program, you might find it overwhelming at first. That is okay, and to be expected. Programming is best learned by doing, doing repeatedly, and looking up how to do. Keep going, ask questions, and refer often to the resources provided in this course.\nStop and ask questions anytime! I am gearing this material to new learners of R but as an advanced user myself it is possible I can overlook something that I need to explain in more detail. Please stop me and ask if anything is unclear!\nSome of the topics covered here will be more or less useful to each of you, depending on the type of work you do and how much and what type of data are involved. But all of it will help you have a strong foundation even if the specific topics here are not directly applicable in your field.\n\n\n\n\nR and RStudio are two separate things, and to use both you will need to install two programs onto your personal computer.\n\n\n\n\n\n\n\nR is a free, open-source software environment for statistical computing and graphics\nMany of the functions you may wish to use will be contributed by other users as packages and available through repositories such as CRAN, GitHub, or Bioconductor, among others\nIt is your responsibility to vet the quality and accuracy of any user-contributed packages\nThe functions available with the initial installation of R, known as base R, can be considered trustworthy\n\n\n\n\n\n\n\nInstalling R\n\n\n\n\nGo to the website for The Comprehensive R Archive Network.\nThe top of the web page provides three links for downloading R. Follow the link that describes your operating system: Windows, Mac, or Linux.\nSelect the appropriate operating system 4a. Windows: Then click “base”, then click “Download R-x.y.z for Windows” where x.y.z corresponds to the current version of R, then double click the .exe file that downloads and follow the prompts 4b. Mac: Then click the option corresponding to your Mac model, then double click the .pkg file that downloads and follow the prompts\n\n\n\n\n\n\n\n\n\n\nRStudio is an Integrated Development Environment (IDE).\nIt runs R and allows users to develop and edit programs and offers higher quality graphics and a more user-friendly interface.\nNote that RStudio is not a standalone program, you must have a separate installation of R\n\n\n\n\n\n\n\nInstalling RStudio\n\n\n\n\nGo to the website for RStudio\nSelect “Download RStudio Desktop” under “Open Source Edition”\nClick the button for “Download RStudio”\nScroll down and select the appropriate version for your operating system\nAn installer will download - double click on it and follow the prompts\n\n\n\n\n\n\n\n\n\n\nWe will primarily use Posit Workbench on the servers, where the R version and many R packages are updated regularly\nSee the wiki for details: http://jjnb-wiki-v-00.bio.ri.ccf.org/index.php/Running_R\n\n\nLogin in using your Linux credentials at one of the links, for example lri-r07: https://lri-r07.lerner.ccf.org/auth-sign-in\n\n\n\n\n\nSelect “New Session”\n\n\n\n\n\nSelect “RStudio Pro” then click “Start Session”\n\n\n\n\n\nYou will have some version of the below appear. Note that it will not be identical to this as I have changed many options over time for my personal preferences.\n\n\n\n\nThe layout of the panes can be customized by going to Tools &gt; Global Options &gt; Pane Layout. I can help you customize as well if there are things you prefer, just ask!\n\n\n\nRstudio panes:\n\nText editor (i.e. R script) - this is where you will type your code, and you will save this file to a project folder for reproducibility\nConsole - this is where the code will be executed\nOther panes will contain a variety of tabs. Some to note include:\n\nEnvironment: where you can see objects and data files that are available in your current session\nFiles: here you should be able to access all folders and files on your home drive\nPlots: this is where plots will disply\nHelp: this is where you will get help files for R functions\nViewer: this is where you would preview any html output like a gt table or Quarto document\n\n\nUsing the text editor in RStudio:\nAlways use a text editor to type code so that it can be saved for reproducibility purposes.\nHow to open a text editor window:\n\nTo open a new text editor window go to: File &gt; New File &gt; R Script\nTo save the file go to: File &gt; Save\n\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 1\n\n\n\n\n\n\nLogin to Posit Workbench and start a new RStudio session.\nGo to File &gt; New File &gt; R Script to open a new, blank R script\nSave the file to a new folder on your home directory called “mmedr” with the name “session1-exercises.R”\n\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 1 Solution"
  },
  {
    "objectID": "tools-introduction.html#introduction-to-the-tools",
    "href": "tools-introduction.html#introduction-to-the-tools",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "R and RStudio are two separate things, and to use both you will need to install two programs onto your personal computer."
  },
  {
    "objectID": "tools-introduction.html#r",
    "href": "tools-introduction.html#r",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "R is a free, open-source software environment for statistical computing and graphics\nMany of the functions you may wish to use will be contributed by other users as packages and available through repositories such as CRAN, GitHub, or Bioconductor, among others\nIt is your responsibility to vet the quality and accuracy of any user-contributed packages\nThe functions available with the initial installation of R, known as base R, can be considered trustworthy\n\n\n\n\n\n\n\nInstalling R\n\n\n\n\nGo to the website for The Comprehensive R Archive Network.\nThe top of the web page provides three links for downloading R. Follow the link that describes your operating system: Windows, Mac, or Linux.\nSelect the appropriate operating system 4a. Windows: Then click “base”, then click “Download R-x.y.z for Windows” where x.y.z corresponds to the current version of R, then double click the .exe file that downloads and follow the prompts 4b. Mac: Then click the option corresponding to your Mac model, then double click the .pkg file that downloads and follow the prompts"
  },
  {
    "objectID": "tools-introduction.html#rstudio",
    "href": "tools-introduction.html#rstudio",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "RStudio is an Integrated Development Environment (IDE).\nIt runs R and allows users to develop and edit programs and offers higher quality graphics and a more user-friendly interface.\nNote that RStudio is not a standalone program, you must have a separate installation of R\n\n\n\n\n\n\n\nInstalling RStudio\n\n\n\n\nGo to the website for RStudio\nSelect “Download RStudio Desktop” under “Open Source Edition”\nClick the button for “Download RStudio”\nScroll down and select the appropriate version for your operating system\nAn installer will download - double click on it and follow the prompts"
  },
  {
    "objectID": "tools-introduction.html#posit-workbench",
    "href": "tools-introduction.html#posit-workbench",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "We will primarily use Posit Workbench on the servers, where the R version and many R packages are updated regularly\nSee the wiki for details: http://jjnb-wiki-v-00.bio.ri.ccf.org/index.php/Running_R\n\n\nLogin in using your Linux credentials at one of the links, for example lri-r07: https://lri-r07.lerner.ccf.org/auth-sign-in\n\n\n\n\n\nSelect “New Session”\n\n\n\n\n\nSelect “RStudio Pro” then click “Start Session”\n\n\n\n\n\nYou will have some version of the below appear. Note that it will not be identical to this as I have changed many options over time for my personal preferences.\n\n\n\n\nThe layout of the panes can be customized by going to Tools &gt; Global Options &gt; Pane Layout. I can help you customize as well if there are things you prefer, just ask!"
  },
  {
    "objectID": "tools-introduction.html#using-rstudio",
    "href": "tools-introduction.html#using-rstudio",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "Rstudio panes:\n\nText editor (i.e. R script) - this is where you will type your code, and you will save this file to a project folder for reproducibility\nConsole - this is where the code will be executed\nOther panes will contain a variety of tabs. Some to note include:\n\nEnvironment: where you can see objects and data files that are available in your current session\nFiles: here you should be able to access all folders and files on your home drive\nPlots: this is where plots will disply\nHelp: this is where you will get help files for R functions\nViewer: this is where you would preview any html output like a gt table or Quarto document\n\n\nUsing the text editor in RStudio:\nAlways use a text editor to type code so that it can be saved for reproducibility purposes.\nHow to open a text editor window:\n\nTo open a new text editor window go to: File &gt; New File &gt; R Script\nTo save the file go to: File &gt; Save"
  },
  {
    "objectID": "tools-introduction.html#exercises",
    "href": "tools-introduction.html#exercises",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "Session 1, Exercise 1\n\n\n\n\n\n\nLogin to Posit Workbench and start a new RStudio session.\nGo to File &gt; New File &gt; R Script to open a new, blank R script\nSave the file to a new folder on your home directory called “mmedr” with the name “session1-exercises.R”\n\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 1 Solution"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R For Molecular Medicine",
    "section": "",
    "text": "This is the new website to host R teaching materials for the Cleveland Clinic Molecular Medicine PhD students. It is in the very early stages of development and very much a work in progress.\nThe material will be split across 5 1-hour sessions as part of the Tools course, followed by 2 2-hour sessions as part of the Biostatistics and Epidemiology course."
  },
  {
    "objectID": "index.html#tools-course",
    "href": "index.html#tools-course",
    "title": "R For Molecular Medicine",
    "section": "Tools course",
    "text": "Tools course\n\nSession 1\n\nIntroduction to the tools\n\nR\nRStudio\nPosit Workbench\n\nInstalling and loading R packages\n\nCRAN\nGithub\nBioconductor\n\nLoading data\n\n.xlsx\n.csv\nOther formats\nVariable names (janitor)\n\nReproducibility\n\nR projects\nhere package\nProject workflow and organization\n\n\n\n\nSession 2\n\nBasic programming\n\nExecuting code\nGetting help\nThe pipe operator\nCase sensitive\nAssigning objects\nFunctions\nVectors and sequences\nStrings\nTesting for equality\n\n\n\n\nSession 3\n\nManipulate dataframes\n\nIndexing operators - extract rows/columns\nAccessing variables in datasets\nSubsetting data\nRename columns\nAdd columns\nGroup a continuous variable\nRecategorize a categorical variable\nSort by row\nReshape (wide to long, long to wide)\n\n\n\n\nSession 4\n\nDescriptive statistics\n\nOne-way frequency table\nTwo-way contingency table\nSummary statistics\nSummary statistics by group\n\n\n\n\nSession 5\n\nPut it all together"
  },
  {
    "objectID": "index.html#biostatistics-and-epidemiology",
    "href": "index.html#biostatistics-and-epidemiology",
    "title": "R For Molecular Medicine",
    "section": "Biostatistics and Epidemiology",
    "text": "Biostatistics and Epidemiology\n\nSession 1\n\nRefresher of the Tools course material\n\nR basics\nDescriptive statistics\n\nVisualization\n\nScatterplot\nBar chart\nHistogram\nLine chart\nBoxplot\nCustomization\nFaceting\nSaving plots\n\n\n\n\nSession 2\n\nAdjustment for multiple comparisons\nAdvanced programming\n\nWriting custom functions\nFor loops/apply/map\n\nStatistical tests\n\nFisher’s exact test\nChi-squared test\nWilcoxon rank sum test/Kruskal-Wallis test\nT-test\nANOVA\nWilcoxon signed rank test\nPaired t-test"
  },
  {
    "objectID": "bio-epi-statistical-tests.html",
    "href": "bio-epi-statistical-tests.html",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "In this session, we will introduce methods to adjust p-values to account for multiple testing, learn advanced programming techniques including for loops and writing custom functions, and cover basic statistical tests to conduct hypothesis testing for different combinations of continuous, categorical, and paired data.\n\n\n\n\nThere are two main tests for associations between two categorical variables: the chi-squared test and Fisher’s exact test.\nConduct these tests in R using the following functions:\n\nchisq.test()\nfisher.test()\n\nConduct a chi-squared test of the null hypothesis that there is no association between treatment and response versus the alternative hypothesis that there is an association between treatment and response.\n\nlibrary(gtsummary)\n\nchisq.test(x = trial$trt, y = trial$response)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  trial$trt and trial$response\nX-squared = 0.22329, df = 1, p-value = 0.6365\n\n\nThe chi-squared test statistic is 0.22329 with associated p-value 0.6365.\nDo not reject the null hypothesis since the p-value is greater than the traditional threshold for significance of 0.05.\nConduct a Fisher’s exact test (an alternative to the chi-squared test when any expected cell count is &lt;5):\n\nfisher.test(x = trial$trt, y = trial$response)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  trial$trt and trial$response\np-value = 0.5403\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.6326222 2.3394994\nsample estimates:\nodds ratio \n  1.213605 \n\n\nThe p-value is 0.5403.\nDo not reject the null hypothesis of no association between treatment and response since the p-value is greater than the traditional threshold for significance of 0.05.\n\n\n\nThe most common statistical tests for the association between a continuous and a categorical variable are the non-parametric Kruskal-Wallis test and the parametric t-test.\nConduct these tests in R using the following functions:\n\nkruskal.test() (see also wilcox.test())\nt.test()\n\nNote that the Kruskal-Wallis test is also known as the Wilcoxon rank-sum test in the special case of a 2-level categorical variable.\n“Non-parametric” means that there is no parametric distribution assumption made on the continuous variable - i.e. the continuous variable does not need to be normally distributed.\nConduct a Kruskal-Wallis test to test the null hypothesis of no association between marker status and response versus the alternative hypothesis that there is an association between marker status and response.\n\nkruskal.test(marker ~ response, data = trial)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  marker by response\nKruskal-Wallis chi-squared = 2.727, df = 1, p-value = 0.09866\n\n\nSince “response” is a 2-level categorical variable, the Wilcoxon rank-sum test (without continuity correction) produces the same result:\n\nwilcox.test(marker ~ response, correct = FALSE, data = trial)\n\n\n    Wilcoxon rank sum test\n\ndata:  marker by response\nW = 3043, p-value = 0.09866\nalternative hypothesis: true location shift is not equal to 0\n\n\nWith a p-value of 0.099, do not reject the null hypothesis at the 0.05 significance level.\n“Parametric” means that the test assumes a parametric distirbution - in the case of the t-test, it relies on the assumption that the continuous variable is normally distributed.\nIf appropriate given the distribution of the continuous variable, conduct a t-test:\n\nt.test(marker ~ response, data = trial)\n\n\n    Welch Two Sample t-test\n\ndata:  marker by response\nt = -1.5851, df = 96.232, p-value = 0.1162\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.51378857  0.05753795\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8425238       1.0706491 \n\n\nThe p-value differs but the conclusion is the same: do not reject the null hypothesis that there is no association between marker and response.\nHowever, in this example the data are right-skewed so a t-test is not appropriate.\n\nlibrary(ggplot2)\n\nggplot(data = trial, aes(x = marker)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nTips:\n\nFor the preceding two tests we used the formula option to supply the information for the test\nA formula in R looks like LHS ~ RHS where the left-hand side (“LHS”) is typically an outcome variable or dependent variable and the right-hand side (“RHS”) is the independent variable.\nFor multivariable regression, you can specify formulas in the format Y ~ X1 + X2 + X3... when there is more than one independent variable.\nYou must also supply the name of the dataset to the data = argument so that R knows where to look to find the specified variables.\nSee the help files for alternative ways to supply the information for each test\n\nWhat if our categorical variables has more than 2 levels, but we want to do a parametric test? Then we will use ANOVA. ANOVA tests the null hypothesis of no association between marker and grade, which has more than two levels so a t-test is not appropriate, and relies on the assumption that the continuous variable is normally distributed.\n\nlm_mod &lt;- lm(marker ~ grade, data = trial)\nanova(lm_mod)\n\nAnalysis of Variance Table\n\nResponse: marker\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngrade       2   5.385 2.69264  3.7529 0.02523 *\nResiduals 187 134.168 0.71748                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd here we would reject the null hypothesis of no association between marker and grade, at the 0.05 level.\n\n\n\nNext we consider the setting where we have paired data. Paired data are any data where a continuous variable is matched in a meaningful way between the two groups. Examples:\n\nPre vs post drug/surgery/test/etc on a single subject\nMeasurements on two eyes within subject\nSibling or spousal pairs\n\nWe will use the example data from the help page found by running ?wilcox.test. These are measurements of a continuous depression scale on 9 patients taken at the first (x) and second (y) visits after inititiation of a certain therapy. We want to test the null hypothesis that there is no difference in depression scale values between these two times.\n\nx &lt;- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny &lt;- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\n\nThe Wilcoxon signed-rank test is a non-parametric test for the association between paired continuous data. Note that this is the same function we used for the Wilcoxon rank-sum test previously, with non-paired data, only now we add the paired = TRUE argument.\n\nwilcox.test(x, y, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.03906\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe reject the null hypothesis at the 0.05 level.\nFor demonstration purposes we also try the paired t-test, which requires data to be normally distributed, to test the null hypothesis that there is no difference in depression scale values between these two times. Again, note that this is the same function we used for the standard t-test previously, only now we have added the paired = TRUE argument.\n\nt.test(x, y, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  x and y\nt = 3.0354, df = 8, p-value = 0.01618\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1037787 0.7599991\nsample estimates:\nmean difference \n      0.4318889 \n\n\nAnd again we reject the null hypothesis at the 0.05 level."
  },
  {
    "objectID": "bio-epi-statistical-tests.html#statistical-tests",
    "href": "bio-epi-statistical-tests.html#statistical-tests",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "There are two main tests for associations between two categorical variables: the chi-squared test and Fisher’s exact test.\nConduct these tests in R using the following functions:\n\nchisq.test()\nfisher.test()\n\nConduct a chi-squared test of the null hypothesis that there is no association between treatment and response versus the alternative hypothesis that there is an association between treatment and response.\n\nlibrary(gtsummary)\n\nchisq.test(x = trial$trt, y = trial$response)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  trial$trt and trial$response\nX-squared = 0.22329, df = 1, p-value = 0.6365\n\n\nThe chi-squared test statistic is 0.22329 with associated p-value 0.6365.\nDo not reject the null hypothesis since the p-value is greater than the traditional threshold for significance of 0.05.\nConduct a Fisher’s exact test (an alternative to the chi-squared test when any expected cell count is &lt;5):\n\nfisher.test(x = trial$trt, y = trial$response)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  trial$trt and trial$response\np-value = 0.5403\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.6326222 2.3394994\nsample estimates:\nodds ratio \n  1.213605 \n\n\nThe p-value is 0.5403.\nDo not reject the null hypothesis of no association between treatment and response since the p-value is greater than the traditional threshold for significance of 0.05.\n\n\n\nThe most common statistical tests for the association between a continuous and a categorical variable are the non-parametric Kruskal-Wallis test and the parametric t-test.\nConduct these tests in R using the following functions:\n\nkruskal.test() (see also wilcox.test())\nt.test()\n\nNote that the Kruskal-Wallis test is also known as the Wilcoxon rank-sum test in the special case of a 2-level categorical variable.\n“Non-parametric” means that there is no parametric distribution assumption made on the continuous variable - i.e. the continuous variable does not need to be normally distributed.\nConduct a Kruskal-Wallis test to test the null hypothesis of no association between marker status and response versus the alternative hypothesis that there is an association between marker status and response.\n\nkruskal.test(marker ~ response, data = trial)\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  marker by response\nKruskal-Wallis chi-squared = 2.727, df = 1, p-value = 0.09866\n\n\nSince “response” is a 2-level categorical variable, the Wilcoxon rank-sum test (without continuity correction) produces the same result:\n\nwilcox.test(marker ~ response, correct = FALSE, data = trial)\n\n\n    Wilcoxon rank sum test\n\ndata:  marker by response\nW = 3043, p-value = 0.09866\nalternative hypothesis: true location shift is not equal to 0\n\n\nWith a p-value of 0.099, do not reject the null hypothesis at the 0.05 significance level.\n“Parametric” means that the test assumes a parametric distirbution - in the case of the t-test, it relies on the assumption that the continuous variable is normally distributed.\nIf appropriate given the distribution of the continuous variable, conduct a t-test:\n\nt.test(marker ~ response, data = trial)\n\n\n    Welch Two Sample t-test\n\ndata:  marker by response\nt = -1.5851, df = 96.232, p-value = 0.1162\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.51378857  0.05753795\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8425238       1.0706491 \n\n\nThe p-value differs but the conclusion is the same: do not reject the null hypothesis that there is no association between marker and response.\nHowever, in this example the data are right-skewed so a t-test is not appropriate.\n\nlibrary(ggplot2)\n\nggplot(data = trial, aes(x = marker)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nTips:\n\nFor the preceding two tests we used the formula option to supply the information for the test\nA formula in R looks like LHS ~ RHS where the left-hand side (“LHS”) is typically an outcome variable or dependent variable and the right-hand side (“RHS”) is the independent variable.\nFor multivariable regression, you can specify formulas in the format Y ~ X1 + X2 + X3... when there is more than one independent variable.\nYou must also supply the name of the dataset to the data = argument so that R knows where to look to find the specified variables.\nSee the help files for alternative ways to supply the information for each test\n\nWhat if our categorical variables has more than 2 levels, but we want to do a parametric test? Then we will use ANOVA. ANOVA tests the null hypothesis of no association between marker and grade, which has more than two levels so a t-test is not appropriate, and relies on the assumption that the continuous variable is normally distributed.\n\nlm_mod &lt;- lm(marker ~ grade, data = trial)\nanova(lm_mod)\n\nAnalysis of Variance Table\n\nResponse: marker\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \ngrade       2   5.385 2.69264  3.7529 0.02523 *\nResiduals 187 134.168 0.71748                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd here we would reject the null hypothesis of no association between marker and grade, at the 0.05 level.\n\n\n\nNext we consider the setting where we have paired data. Paired data are any data where a continuous variable is matched in a meaningful way between the two groups. Examples:\n\nPre vs post drug/surgery/test/etc on a single subject\nMeasurements on two eyes within subject\nSibling or spousal pairs\n\nWe will use the example data from the help page found by running ?wilcox.test. These are measurements of a continuous depression scale on 9 patients taken at the first (x) and second (y) visits after inititiation of a certain therapy. We want to test the null hypothesis that there is no difference in depression scale values between these two times.\n\nx &lt;- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)\ny &lt;- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)\n\nThe Wilcoxon signed-rank test is a non-parametric test for the association between paired continuous data. Note that this is the same function we used for the Wilcoxon rank-sum test previously, with non-paired data, only now we add the paired = TRUE argument.\n\nwilcox.test(x, y, paired = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  x and y\nV = 40, p-value = 0.03906\nalternative hypothesis: true location shift is not equal to 0\n\n\nWe reject the null hypothesis at the 0.05 level.\nFor demonstration purposes we also try the paired t-test, which requires data to be normally distributed, to test the null hypothesis that there is no difference in depression scale values between these two times. Again, note that this is the same function we used for the standard t-test previously, only now we have added the paired = TRUE argument.\n\nt.test(x, y, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  x and y\nt = 3.0354, df = 8, p-value = 0.01618\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.1037787 0.7599991\nsample estimates:\nmean difference \n      0.4318889 \n\n\nAnd again we reject the null hypothesis at the 0.05 level."
  },
  {
    "objectID": "bio-epi-multiple-testing.html",
    "href": "bio-epi-multiple-testing.html",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "In this session, we will introduce methods to adjust p-values to account for multiple testing, learn advanced programming techniques including for loops and writing custom functions, and cover basic statistical tests to conduct hypothesis testing for different combinations of continuous, categorical, and paired data.\n\n\nThe problem is referred to as multiple comparisons, multiple testing, or multiplicity, but they all mean the same thing.\nWhat do we mean by multiple comparisons, and what is the issue?\nWhen multiple statistical tests are conducted simultaneously, type I errors become more likely. Therefore, our standard type I error rate of 0.05 that is used to determine whether p-values are significant or not is no longer correct, because the type I error has been inflated due to the multiple testing.\nMultiple comparisons affects both p-values and confidence intervals.\nPrior to significance testing we need to identify a more strict p-value threshold or, alternatively, directly adjust our p-values.\nA number of corrections for multiple comparisons can be implemented with the R function p.adjust().\nConsider the setting where we have p-values for the association between 10 different gene mutations and treatment response:\n\nlibrary(tibble)\nlibrary(gt)\n\nptab &lt;-\n  tibble(\n  Gene = paste0(\"gene\", seq(1:10)),\n  `p-value` = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, \n                0.802)\n  ) \n\nptab |&gt; \n  gt() |&gt; \n  tab_header(\"Table of p-values for association with treatment response\")\n\n\n\n\n\n\n\nTable of p-values for association with treatment response\n\n\nGene\np-value\n\n\n\n\ngene1\n0.001\n\n\ngene2\n0.245\n\n\ngene3\n0.784\n\n\ngene4\n0.034\n\n\ngene5\n0.004\n\n\ngene6\n0.123\n\n\ngene7\n0.089\n\n\ngene8\n0.063\n\n\ngene9\n0.228\n\n\ngene10\n0.802\n\n\n\n\n\n\n\nFirst, adjust these for multiple testing using the false-discovery rate approach. Pass the vector of p-values to p.adjust() and specify method = \"fdr\":\n\np.adjust(\n  p = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, 0.802),\n  method = \"fdr\"\n)\n\n [1] 0.0100000 0.3062500 0.8020000 0.1133333 0.0200000 0.2050000 0.1780000\n [8] 0.1575000 0.3062500 0.8020000\n\n\nGet back a vector of the adjusted p-values, listed in the same order as the originally provided p-values.\nThe false-discovery rate is the expected proportion of false positives among all significant tests, and is an appropriate method to use when a study is viewed as exploratory and significant results will be followed up in an independent study.\nAlternatively, we could adjust the p-values for multiple testing using the family-wise error approach. Some options include the Bonferroni correction (most conservative, i.e. most difficult to achieve significance) (method = \"bonferroni\") and the Holm-Bonferroni correction (method = \"holm\").\n\np.adjust(\n  p = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, 0.802),\n  method = \"bonferroni\"\n)\n\n [1] 0.01 1.00 1.00 0.34 0.04 1.00 0.89 0.63 1.00 1.00\n\n\nAnd we see that using the Bonferroni method, the adjusted p-values are larger than when using the FDR method. The familywise error rate is the probability of making a type I error among a specified group (“family”) of tests.\nAfter adjusting the p-values, we can compare them to the standard 0.05 level of significance.\nWe can place the FDR-adjusted p-values into our table by directly applying the p.adjust() function to our column of p-values as follows:\n\nlibrary(dplyr)\n\nptab |&gt; \n  mutate(\n    `q-value` = p.adjust(`p-value`, method = \"fdr\")\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(columns = `q-value`, decimals = 3) |&gt; \n  tab_header(\"Table of adjusted p-values for association with treatment response\")\n\n\n\n\n\n\n\nTable of adjusted p-values for association with treatment response\n\n\nGene\np-value\nq-value\n\n\n\n\ngene1\n0.001\n0.010\n\n\ngene2\n0.245\n0.306\n\n\ngene3\n0.784\n0.802\n\n\ngene4\n0.034\n0.113\n\n\ngene5\n0.004\n0.020\n\n\ngene6\n0.123\n0.205\n\n\ngene7\n0.089\n0.178\n\n\ngene8\n0.063\n0.158\n\n\ngene9\n0.228\n0.306\n\n\ngene10\n0.802\n0.802\n\n\n\n\n\n\n\nNote that “q-value” is a common term for p-values that have been adjusted for multiple comparisons."
  },
  {
    "objectID": "bio-epi-multiple-testing.html#adjustment-for-multiple-comparisons",
    "href": "bio-epi-multiple-testing.html#adjustment-for-multiple-comparisons",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "The problem is referred to as multiple comparisons, multiple testing, or multiplicity, but they all mean the same thing.\nWhat do we mean by multiple comparisons, and what is the issue?\nWhen multiple statistical tests are conducted simultaneously, type I errors become more likely. Therefore, our standard type I error rate of 0.05 that is used to determine whether p-values are significant or not is no longer correct, because the type I error has been inflated due to the multiple testing.\nMultiple comparisons affects both p-values and confidence intervals.\nPrior to significance testing we need to identify a more strict p-value threshold or, alternatively, directly adjust our p-values.\nA number of corrections for multiple comparisons can be implemented with the R function p.adjust().\nConsider the setting where we have p-values for the association between 10 different gene mutations and treatment response:\n\nlibrary(tibble)\nlibrary(gt)\n\nptab &lt;-\n  tibble(\n  Gene = paste0(\"gene\", seq(1:10)),\n  `p-value` = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, \n                0.802)\n  ) \n\nptab |&gt; \n  gt() |&gt; \n  tab_header(\"Table of p-values for association with treatment response\")\n\n\n\n\n\n\n\nTable of p-values for association with treatment response\n\n\nGene\np-value\n\n\n\n\ngene1\n0.001\n\n\ngene2\n0.245\n\n\ngene3\n0.784\n\n\ngene4\n0.034\n\n\ngene5\n0.004\n\n\ngene6\n0.123\n\n\ngene7\n0.089\n\n\ngene8\n0.063\n\n\ngene9\n0.228\n\n\ngene10\n0.802\n\n\n\n\n\n\n\nFirst, adjust these for multiple testing using the false-discovery rate approach. Pass the vector of p-values to p.adjust() and specify method = \"fdr\":\n\np.adjust(\n  p = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, 0.802),\n  method = \"fdr\"\n)\n\n [1] 0.0100000 0.3062500 0.8020000 0.1133333 0.0200000 0.2050000 0.1780000\n [8] 0.1575000 0.3062500 0.8020000\n\n\nGet back a vector of the adjusted p-values, listed in the same order as the originally provided p-values.\nThe false-discovery rate is the expected proportion of false positives among all significant tests, and is an appropriate method to use when a study is viewed as exploratory and significant results will be followed up in an independent study.\nAlternatively, we could adjust the p-values for multiple testing using the family-wise error approach. Some options include the Bonferroni correction (most conservative, i.e. most difficult to achieve significance) (method = \"bonferroni\") and the Holm-Bonferroni correction (method = \"holm\").\n\np.adjust(\n  p = c(0.001, 0.245, 0.784, 0.034, 0.004, 0.123, 0.089, 0.063, 0.228, 0.802),\n  method = \"bonferroni\"\n)\n\n [1] 0.01 1.00 1.00 0.34 0.04 1.00 0.89 0.63 1.00 1.00\n\n\nAnd we see that using the Bonferroni method, the adjusted p-values are larger than when using the FDR method. The familywise error rate is the probability of making a type I error among a specified group (“family”) of tests.\nAfter adjusting the p-values, we can compare them to the standard 0.05 level of significance.\nWe can place the FDR-adjusted p-values into our table by directly applying the p.adjust() function to our column of p-values as follows:\n\nlibrary(dplyr)\n\nptab |&gt; \n  mutate(\n    `q-value` = p.adjust(`p-value`, method = \"fdr\")\n  ) |&gt; \n  gt() |&gt; \n  fmt_number(columns = `q-value`, decimals = 3) |&gt; \n  tab_header(\"Table of adjusted p-values for association with treatment response\")\n\n\n\n\n\n\n\nTable of adjusted p-values for association with treatment response\n\n\nGene\np-value\nq-value\n\n\n\n\ngene1\n0.001\n0.010\n\n\ngene2\n0.245\n0.306\n\n\ngene3\n0.784\n0.802\n\n\ngene4\n0.034\n0.113\n\n\ngene5\n0.004\n0.020\n\n\ngene6\n0.123\n0.205\n\n\ngene7\n0.089\n0.178\n\n\ngene8\n0.063\n0.158\n\n\ngene9\n0.228\n0.306\n\n\ngene10\n0.802\n0.802\n\n\n\n\n\n\n\nNote that “q-value” is a common term for p-values that have been adjusted for multiple comparisons."
  },
  {
    "objectID": "bio-epi-advanced.html",
    "href": "bio-epi-advanced.html",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "In this session, we will introduce methods to adjust p-values to account for multiple testing, learn advanced programming techniques including for loops and writing custom functions, and cover basic statistical tests to conduct hypothesis testing for different combinations of continuous, categorical, and paired data.\n\n\n\n\nWe have previously discussed the role of functions in R, and have seen examples of built-in R functions, such as mean() and p.adjust().\nBut sometimes we’ll want to do something that isn’t included in a built-in R function, or that simplifies use of existing functions.\nUser-defined functions are created using the function() function.\nBasic usage is:\n\nfunction(arguments) expression\n\nWhere arguments are arguments you supply to the function and expression is the expression you want to evaluate.\nFor more complicated procedures, you can wrap multiple expressions in curly brackets, and can also specify what value to return using the return() function:\n\nfunction(arguments) {\n  expression1\n  expression2\n  return(value)\n  }\n\nFor example, I always want to show NA values when I look at a contingency table, which means I have to type in the useNA = \"ifany\" arguement every time I use the table() function, since the default in that function is to exclude missing values.\nTo streamline things, I can create a custom function that includes this option:\n\ntabna &lt;- function(x) table(x, useNA = 'ifany')\n\nNow instead of typing:\n\nlibrary(gtsummary)\n\ntable(trial$response, useNA = 'ifany')\n\n\n   0    1 &lt;NA&gt; \n 132   61    7 \n\n\nI can type:\n\ntabna(trial$response)\n\nx\n   0    1 &lt;NA&gt; \n 132   61    7 \n\n\nThis gets particularly useful for long or complex procedures, but is also really useful for short procedures that will be repeated many times - I often use this function 10+ times in a day.\nTry writing a custom function based on the mean() function but including the option to remove NAs from the calculation.\n\n\n\nOften we will want to repeat a set of operations several times, and we can do so using a loop.\nThere are three main types of loops in R:\n\nfor loop\nwhile loop\nrepeat loop\n\nWe will focus on the for loop today.\nHere is a basic example using the print() function to repeatedly print a value:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere are the steps of the execution:\n\nThe value of i is set to 1\nThe value of i is printed to the console (first iteration complete)\nThe value of i is set to 2 (the for loop loops back to the beginning)\nThe value of i is printed to the console\n\nAnd so on until we reach the last value of i, and the process is complete.\nSay you have a biomarker in your dataset but you know the machine that generated the data has a lower limit of detection of 0.2. You could choose to impute half the detection limit for any values that fall below 0.2 as follows:\n\ntrial$marker_corrected &lt;- trial$marker\n\nfor(i in 1:nrow(trial)) {\n  if (is.na(trial$marker[i])) {\n    trial$marker_corrected[i] &lt;- NA\n  } else if (trial$marker[i] &lt; 0.2) {\n    trial$marker_corrected[i] &lt;- 0.1\n  }\n}\n\nAnd we can see that our new variable has the value 0.1 for all cases where marker was &lt;0.2:\n\ntrial[trial$marker &lt; 0.2, c(\"marker\", \"marker_corrected\")]\n\n# A tibble: 54 × 2\n   marker marker_corrected\n    &lt;dbl&gt;            &lt;dbl&gt;\n 1  0.16               0.1\n 2  0.144              0.1\n 3  0.06               0.1\n 4  0.128              0.1\n 5  0.157              0.1\n 6  0.066              0.1\n 7  0.096              0.1\n 8  0.105              0.1\n 9  0.043              0.1\n10  0.105              0.1\n# ℹ 44 more rows"
  },
  {
    "objectID": "bio-epi-advanced.html#advanced-programming",
    "href": "bio-epi-advanced.html#advanced-programming",
    "title": "Biostatistics and Epidemiology - Session 2",
    "section": "",
    "text": "We have previously discussed the role of functions in R, and have seen examples of built-in R functions, such as mean() and p.adjust().\nBut sometimes we’ll want to do something that isn’t included in a built-in R function, or that simplifies use of existing functions.\nUser-defined functions are created using the function() function.\nBasic usage is:\n\nfunction(arguments) expression\n\nWhere arguments are arguments you supply to the function and expression is the expression you want to evaluate.\nFor more complicated procedures, you can wrap multiple expressions in curly brackets, and can also specify what value to return using the return() function:\n\nfunction(arguments) {\n  expression1\n  expression2\n  return(value)\n  }\n\nFor example, I always want to show NA values when I look at a contingency table, which means I have to type in the useNA = \"ifany\" arguement every time I use the table() function, since the default in that function is to exclude missing values.\nTo streamline things, I can create a custom function that includes this option:\n\ntabna &lt;- function(x) table(x, useNA = 'ifany')\n\nNow instead of typing:\n\nlibrary(gtsummary)\n\ntable(trial$response, useNA = 'ifany')\n\n\n   0    1 &lt;NA&gt; \n 132   61    7 \n\n\nI can type:\n\ntabna(trial$response)\n\nx\n   0    1 &lt;NA&gt; \n 132   61    7 \n\n\nThis gets particularly useful for long or complex procedures, but is also really useful for short procedures that will be repeated many times - I often use this function 10+ times in a day.\nTry writing a custom function based on the mean() function but including the option to remove NAs from the calculation.\n\n\n\nOften we will want to repeat a set of operations several times, and we can do so using a loop.\nThere are three main types of loops in R:\n\nfor loop\nwhile loop\nrepeat loop\n\nWe will focus on the for loop today.\nHere is a basic example using the print() function to repeatedly print a value:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nHere are the steps of the execution:\n\nThe value of i is set to 1\nThe value of i is printed to the console (first iteration complete)\nThe value of i is set to 2 (the for loop loops back to the beginning)\nThe value of i is printed to the console\n\nAnd so on until we reach the last value of i, and the process is complete.\nSay you have a biomarker in your dataset but you know the machine that generated the data has a lower limit of detection of 0.2. You could choose to impute half the detection limit for any values that fall below 0.2 as follows:\n\ntrial$marker_corrected &lt;- trial$marker\n\nfor(i in 1:nrow(trial)) {\n  if (is.na(trial$marker[i])) {\n    trial$marker_corrected[i] &lt;- NA\n  } else if (trial$marker[i] &lt; 0.2) {\n    trial$marker_corrected[i] &lt;- 0.1\n  }\n}\n\nAnd we can see that our new variable has the value 0.1 for all cases where marker was &lt;0.2:\n\ntrial[trial$marker &lt; 0.2, c(\"marker\", \"marker_corrected\")]\n\n# A tibble: 54 × 2\n   marker marker_corrected\n    &lt;dbl&gt;            &lt;dbl&gt;\n 1  0.16               0.1\n 2  0.144              0.1\n 3  0.06               0.1\n 4  0.128              0.1\n 5  0.157              0.1\n 6  0.066              0.1\n 7  0.096              0.1\n 8  0.105              0.1\n 9  0.043              0.1\n10  0.105              0.1\n# ℹ 44 more rows"
  },
  {
    "objectID": "bio-epi-refresher.html",
    "href": "bio-epi-refresher.html",
    "title": "Biostatistics and Epidemiology - Session 1",
    "section": "",
    "text": "In this session, we will review the basics of R and generating descriptive statistics in R, which were introduced in the Tools course. Then we will learn about data visualization, including creating scatterplots, bar charts, histograms, line charts, and boxplots. We’ll discuss plot customization, faceting, and saving plots. Both univariate and bivariable plotting will be covered.\n\n\n\n\nFirst, we’ll review the basics of R and R programming.\n\n\n\n\n\n\nR is a free, open-source software environment for statistical computing and graphics\nMany of the functions you may wish to use will be contributed by other users as packages and available through repositories such as CRAN, GitHub, or Bioconductor, among others\nIt is your responsibility to vet the quality and accuracy of any user-contributed packages\nThe functions available with the initial installation of R, known as base R, can be considered trustworthy\n\n\n\n\n\nGo to the website for The Comprehensive R Archive Network.\nThe top of the web page provides three links for downloading R. Follow the link that describes your operating system: Windows, Mac, or Linux.\n\n\n\n\n\n\n\n\nRStudio is an Integrated Development Environment (IDE).\nIt runs R and allows users to develop and edit programs and offers higher quality graphics and a more user-friendly interface.\nNote that RStudio is not a standalone program, you must have a separate installation of R\n\n\n\n\n\nGo to the website for RStudio\nSelect “Download RStudio Desktop” under “Open Source Edition”\nClick the button for “Download RStudio”\nScroll down and select the appropriate version for your operating system\nAn installer will download and provide simple instructions to follow\n\n\n\n\n\n\n\n\nWe will primarily use Posit Workbench on the servers, where the R version and many R packages are updated regularly\nSee the wiki for details: http://jjnb-wiki-v-00.bio.ri.ccf.org/index.php/Running_R\nLogin in using your Linux credentials at one of the links, for example lri-r07: https://lri-r07.lerner.ccf.org/auth-sign-in\n\n\n\n\nWhen you first open RStudio you will see a number of panes:\n\n\n\nThe layout of the panes can be customized by going to Tools &gt; Global Options &gt; Pane Layout.\nPanes:\n\nText editor - this is where you will type your code, and you will save this file to a project folder for reproducibility\nConsole - this is where the code will be executed\nOther panes will contain a variety of tabs. Some to note include:\n\nEnvironment: where you can see objects and data files that are available in your current session\nFiles: here you should be able to access all folders and files on your home drive\nPlots: this is where plots will disply\nHelp: this is where you will get help files for R functions\nViewer: this is where you would preview any html output like a gt table or Quarto document\n\n\n\n\n\nAlways use a text editor to type code so that it can be saved for reproducibility purposes.\nHow to open a text editor window:\n\nTo open a new text editor window go to: File &gt; New File &gt; R script\nTo save the file go to: File &gt; Save\n\n\n\n\nNavigate to https://lri-r07.lerner.ccf.org/auth-sign-in, log in, and run some example code.\nDemonstrate how to:\n\nCreate a new R script\nAdd some code to it\nRun the code\nView help files\nSave the R script\n\n\nx &lt;- c(1, 2, 3, 4)\nmean(x)\nhist(rnorm(100))\n?rnorm\n\n\n\n\nTo send this to the console to be executed, you can do one of the following:\n\nPlace your cursor next to the line you want to run and hit Ctrl+Enter on your keyboard\nPlace your cursor next to the line you want to run and hit the “Run” button\nHighlight all three lines of code and use one of the previous options\n\n\n\n\nGet help by typing ?fnname where fnname is the name of the function of interest.\n\ne.g. to see the help file for the mean function, type ?mean in the console\n??fnname can be used if you aren’t sure of the exact function name - it will return any keyword matches\n\n\n\n\nAfter we have run all three lines of code, we see the results of our mean computation in the Console pane.\nAnd we see the resulting histogram in the Plots pane.\n\n\n\n\n\n\n\n\n\nCRAN is the primary repository for user-contributed R packages.\nPackages that are on CRAN can be installed using the install.packages() function.\nFor example, we can install the {survival} package from CRAN using:\n\ninstall.packages(\"survival\")\n\nGitHub is a common repository for packages that are in development.\nTo install packages from GitHub, first install the {remotes} package from CRAN:\n\ninstall.packages(\"remotes\")\n\nThen, install the GitHub package of interest using install_github(\"username/repository\"). For example, to install the emo repository from the GitHub username hadley, use:\n\nlibrary(remotes)\ninstall_github(\"hadley/emo\")\n\nOr, avoid a call to the library() function by using the syntax library::function():\n\nremotes::install_github(\"hadley/emo\")\n\nBioconductor is a repository for open source code related to biological data. To install packages from Bioconductor, first install the {BiocManager} package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nThen install, for example, the {GenomicFeatures} package using the install function:\n\nBiocManager::install(\"GenomicFeatures\")\n\nInstallation is the first step. Only needs to be done once.\nLoading is the next step. Must be done every time you open a new R session in which you need to use the package.\nThere are two methods for loading R packages:\n\nA call to library() loads the package for your entire R session.\n\n\nlibrary(survival)\nsurvfit(formula, ...)\n\n\nUsing :: accesses the package only for a single function.\n\n\nsurvival::survfit(formula, ...)\n\n\n\n\nThe most common data format we work with are data from Excel. The example trial dataset is available in the {gtsummary} package in R.\n\nInstall and load the “gtsummary” package\nPreview the trial data by simply typing the name of the dataset:\n\n\ninstall.packages(\"gtsummary\")\n\n\nlibrary(gtsummary)\ntrial\n\n# A tibble: 200 × 8\n   trt      age marker stage grade response death ttdeath\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;    &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Drug A    23  0.16  T1    II           0     0    24  \n 2 Drug B     9  1.11  T2    I            1     0    24  \n 3 Drug A    31  0.277 T1    II           0     0    24  \n 4 Drug A    NA  2.07  T3    III          1     1    17.6\n 5 Drug A    51  2.77  T4    III          1     1    16.4\n 6 Drug B    39  0.613 T4    I            0     1    15.6\n 7 Drug A    37  0.354 T1    II           0     0    24  \n 8 Drug A    32  1.74  T1    I            0     1    18.4\n 9 Drug A    31  0.144 T1    II           0     0    24  \n10 Drug B    34  0.205 T3    I            0     1    10.5\n# ℹ 190 more rows\n\n\nThe help page for the trial data can be accessed by running:\n\n?trial\n\nAnd we see the variables and their definitions:\n\n\n\nI have saved this data file out to Excel so that we can practice loading it in R. Go to the course website to download the file named “trial-Excel.xlsx”. Save it somewhere you can find it again.\n\n\n\nThe most common data format we work with are data from Excel.\nData should be:\n\nOne dataset per file\nA single row of column headers across the top\nSimple column names are better - they will get transformed into variable names by R\nTypically one row per patient/sample is ideal\n\nWe will look at options to:\n\nRead in Excel files with {readxl}\nRead in Excel files by converting to .csv first\nRead in other file formats\n\nFirst, install the {readxl} package from CRAN, then load the newly installed package:\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\nThen use the read_excel() function with the appropriate filepath to read in the data and create an object called “mydf”:\n\nmydf &lt;- read_excel(\n  \"~/MMED/MMED501/data/trial-Excel.xlsx\"\n  )\n\nNote that R treats the \\ as a special character so you either need to use / or \\(\\backslash \\backslash\\) in file paths.\nAlso note that on the Linux server (i.e. on Rstudio Pro via Posit Workbench), the path starts at /home/username, so filepaths relative to your directory can start “~/” followed by the folder where the file is located.\nAlternatively, we can convert the file from .xlsx format to .csv format first, and then read it in.\nAdvantages: removes some of the possible formatting pitfalls associated with Excel files, and you don’t need any special packages to read this format.\n\nOpen the Excel file.\nGo to File &gt; Save As and select “CSV (Comma delimited)” from the “Save as type” drop down and save the file to the same location as “trial-csv.csv”\nUse the read.csv() function with the appropriate file path to read in the data and create an object called “mycsv”\n\n\nmycsv &lt;- \n  read.csv(\n    \"~/MMED/MMED501/data/trial-csv.csv\"\n    ) \n\nNote that this is the approach I always use myself and will form the basis of all of my examples\nMany other file formats exist, and here is a non-comprehensive list of functions for loading some of them:\n\nread.table() is the most generic function and can read many file types\nread.csv() is a special case with fixed defaults for comma-separated files\nread.csv2() is a special case with fixed defaults for comma-separated files that were created in a country where commas are used in place of decimal places\nread.delim() is a special case with fixed defaults for tab-delimited files\nread.delim2() is a special case with fixed defaults for tab-delimited files that were created in a country where commas are used in place of decimal places\n\n\n\n\nUse the assignment operator &lt;- to assign values to objects\n\n&lt;- assigns values on the right, to objects on the left\nKeyboard shortcut “Alt” + “-” will insert the assignment operator\n\n\nx &lt;- 55\nx\n\n[1] 55\n\n\nFunctions are pre-packaged scripts that automate more complicated procedures. They are executed by typing the name of the function followed by round brackets. Inside the round brackets we can provide one or more parameters, or arguments:\n\nx &lt;- 144\nsqrt(x)\n\n[1] 12\n\ny &lt;- 123.225\nround(y)\n\n[1] 123\n\nround(y, digits = 1)\n\n[1] 123.2\n\n\nUse c() to create a vector of values or seq() to create a sequence of values:\n\na &lt;- c(1, 2, 3, 4)\nb &lt;- seq(from = 0, to = 100, by = 10)\nb &lt;- seq(0, 100, 10)\nb &lt;- seq(by = 10, to = 100, from = 0)\n\nNote: when we supply the arguments to a function in the order in which they are listed in the documentation, we do not need to name them. If we name them, we can supply them in any order. The above three assignments to b yield the same results.\nHere are all of the possible arguments to the seq() function:\n\nseq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),\n    length.out = NULL, along.with = NULL, ...)\n\nR is case sensitive:\n\ni.e. age is not the same as Age.\nVariable names with spaces are problematic:\n\nDepending on how you read your data in, R may or may not automatically reformat variable names\nIf you end up with a variable called, e.g. “Patient Age” you would need to reference it in backticks: `Patient Age`\nOne option is to use the clean_names() function from the {janitor} package to convert all variable names to snake case (or alternatives):\n\n\n\ninstall.packages(\"janitor\")\njanitor::clean_names(df)\n\nThe == operator tests equality between two values:\n\n5 == 5\n\n[1] TRUE\n\n5 == 9\n\n[1] FALSE\n\n\nThe first returns TRUE because 5 does in fact equal 5.\nThe second returns FALSE because 5 is not equal to 9.\nWe’ll need this later when we subset data.\nR has three main indexing operators:\n\nDollar sign: $\nDouble brackets: [[ ]]\nSingle brackets: [ ]\n\nTo access specific variables, use the $ operator in the form dataframe$varname, where dataframe is the name of the object to which we assigned our data set, and varname is the name of the variable of interest\nFor example, to calculate the mean of the variable age in the dataframe mycsv:\n\nmean(mycsv$age, na.rm = TRUE)\n\n[1] 47.2381\n\n\nNote that we need to add the argument na.rm = TRUE to remove missing values from the calculation of the mean, otherwise NA will be returned if missing values are present\nAlternatively, use double brackets in the form dataframe[[\"varname\"]]\n\nmean(mycsv[[\"age\"]], na.rm = TRUE)\n\n[1] 47.2381\n\n\nSometimes we may want to create a subset of our data, or access a value based on more than one dimension.\nDatasets typically have two dimensions: columns and rows\nFor dataframe df, let i index the row and j index the column.\nThen we can access any single cell in the dataframe using the syntax:\n\ndf[i, j]\n\nWe can use this concept to create subsets of our data as well.\nWe can create a subset of our data based on the values in a row, for example limiting to patients who were treated with Drug A:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\", ]\nnrow(df_sub)\n\n[1] 98\n\n\nWe see that the new data subset has 98 rows.\nThe & operator signifies “and”.\nSo for example we could subset based on patients who were treated with Drug A AND are over 45 years old:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\" & mycsv$age &gt; 45, ]\nnrow(df_sub)\n\n[1] 55\n\n\nAnd we see that the new data subset has 55 rows.\nThe | operator signifies “or”.\nSo for example we could subset based on patients who were treated with Drug A OR are over 45 years old:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\" | mycsv$age &gt; 45, ]\nnrow(df_sub)\n\n[1] 157\n\n\nAnd we see that our new datasubset has 157 rows.\nWe can also create a subset of our data based on columns, for example limiting to trt:\n\ndf_sub &lt;- mycsv[ , c(\"trt\")]\n\nOr we can simultaneously subset based on rows and columns, for example limiting to the trt column among patients with age greater than 45:\n\ndf_sub &lt;- mycsv[mycsv$age &gt; 45, c(\"trt\")]\n\nWe can also subset directly within functions. Suppose we want to calculate the mean of the variable age in the dataframe mycsv, but only among those who were treated with Drug A:\n\nmean(mycsv$age[mycsv$trt == \"Drug A\"], na.rm = TRUE)\n\n[1] 47.01099\n\n\nThis avoids creating additional datasets that may not be needed again.\n\n\n\n\nNext, we’ll review descriptive statistics in R. There are many ways to generate this type of descriptive statistics in R. I will demonstrate one way and if you end up using R a lot you will find what works well for you and the type of data you use.\nWe will be using some functions from the {janitor} package, so make sure you have that installed.\n\ninstall.packages(\"janitor\")\n\n\nlibrary(janitor)\n\nWe can also create very nice tables by also using the {gt} package.\n\ninstall.packages(\"gt\")\n\n\nlibrary(gt)\n\n\n\nFirst, we need to learn about the pipe operator, which we will use to string multiple functions together seamlessly.\nFor example, if we want to take the log transformation of the marker variable and then get the mean, we could nest the two functions as follows:\n\nmean(log(mycsv$marker), na.rm = TRUE)\n\n[1] -0.707878\n\n\nOr we can connect them with the pipe operator:\n\nmycsv$marker |&gt; \n  log() |&gt; \n  mean(na.rm = TRUE)\n\n[1] -0.707878\n\n\nThe left hand side is passed as the first argument to the function on the right hand side.\nYou can use the native pipe operator through the keyboard shortcut ctrl + shift + m.\nThis creates code that is very readable and concise, and easy to comment out various parts if needed.\nWe’ll be using this throughout the R sessions in this course.\n\n\n\nMake sure your variable of interest is in its own column in your dataframe, then use the tabyl() function from {janitor}:\n\nmycsv |&gt; \n  tabyl(trt) |&gt; \n  adorn_pct_formatting()\n\n    trt   n percent\n Drug A  98   49.0%\n Drug B 102   51.0%\n\n\nAnd we get a frequency table for the trt variable, with the percentages formatted using the adorn_pct_formatting() function.\n\n\n\nLet’s create a table with trt on the rows and response on the columns. The most basic table is created as:\n\nmycsv |&gt; \n  tabyl(trt, response)\n\n    trt  0  1 NA_\n Drug A 67 28   3\n Drug B 65 33   4\n\n\nThen we can use the {gt} package and it’s associated features to customize our two-way contingency table. See the {gt} package website for details.\nLet’s label the row variable and column variable, and add a title:\n\nmycsv |&gt; \n  tabyl(trt, response) |&gt; \n  gt(\n    rowname_col = \"trt\" \n  ) |&gt; \n  tab_spanner(          \n    columns = 2:4,\n    label = \"Response?\"\n  ) |&gt; \n  tab_stubhead(         \n    label = \"Treatment\" \n  ) |&gt; \n  tab_header(           \n    title = \"Response according to treatment group\"\n  )\n\n\n\n\n\n\n\nResponse according to treatment group\n\n\nTreatment\nResponse?\n\n\n0\n1\nNA_\n\n\n\n\nDrug A\n67\n28\n3\n\n\nDrug B\n65\n33\n4\n\n\n\n\n\n\n\nAlternatively, we could display percentages in our table:\n\nmycsv |&gt; \n  tabyl(trt, response) |&gt; \n  adorn_percentages(denominator = \"all\") |&gt; \n  gt() |&gt; \n  fmt_percent(\n    columns = -1,\n    decimals = 1\n  ) |&gt; \n  tab_spanner(          \n    columns = 2:4,\n    label = \"Response?\"\n  ) |&gt; \n  tab_stubhead(         \n    label = \"Treatment\" \n  ) |&gt; \n  tab_header(           \n    title = \"Response according to treatment group\"\n  )\n\n\n\n\n\n\n\nResponse according to treatment group\n\n\ntrt\nResponse?\n\n\n0\n1\nNA_\n\n\n\n\nDrug A\n33.5%\n14.0%\n1.5%\n\n\nDrug B\n32.5%\n16.5%\n2.0%\n\n\n\n\n\n\n\n\n\n\nWe will use the summarize() function from the {dplyr} package to compute summary statistics.\n\ninstall.packages(\"dplyr\")\n\n\nlibrary(dplyr)\n\nLet’s compute the mean and standard deviation of age and marker:\n\nmycsv |&gt; \n  summarize(\n    avg_age = mean(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    avg_marker = mean(marker, na.rm = TRUE),\n    sd_marker = sd(marker, na.rm = TRUE)\n  )\n\n  avg_age   sd_age avg_marker sd_marker\n1 47.2381 14.31193  0.9159895 0.8592891\n\n\n\n\n\nAnd we can easily extend this code to generate summary statistics by group by using the group_by() function from the {dplyr} package.\nLet’s get the same summary table for age and marker, but by trt:\n\nmycsv |&gt; \n  group_by(trt) |&gt; \n  summarize(\n    avg_age = mean(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    avg_marker = mean(marker, na.rm = TRUE),\n    sd_marker = sd(marker, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 5\n  trt    avg_age sd_age avg_marker sd_marker\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Drug A    47.0   14.7      1.02      0.885\n2 Drug B    47.4   14.0      0.821     0.828"
  },
  {
    "objectID": "bio-epi-refresher.html#refresher-of-the-tools-course-material",
    "href": "bio-epi-refresher.html#refresher-of-the-tools-course-material",
    "title": "Biostatistics and Epidemiology - Session 1",
    "section": "",
    "text": "First, we’ll review the basics of R and R programming.\n\n\n\n\n\n\nR is a free, open-source software environment for statistical computing and graphics\nMany of the functions you may wish to use will be contributed by other users as packages and available through repositories such as CRAN, GitHub, or Bioconductor, among others\nIt is your responsibility to vet the quality and accuracy of any user-contributed packages\nThe functions available with the initial installation of R, known as base R, can be considered trustworthy\n\n\n\n\n\nGo to the website for The Comprehensive R Archive Network.\nThe top of the web page provides three links for downloading R. Follow the link that describes your operating system: Windows, Mac, or Linux.\n\n\n\n\n\n\n\n\nRStudio is an Integrated Development Environment (IDE).\nIt runs R and allows users to develop and edit programs and offers higher quality graphics and a more user-friendly interface.\nNote that RStudio is not a standalone program, you must have a separate installation of R\n\n\n\n\n\nGo to the website for RStudio\nSelect “Download RStudio Desktop” under “Open Source Edition”\nClick the button for “Download RStudio”\nScroll down and select the appropriate version for your operating system\nAn installer will download and provide simple instructions to follow\n\n\n\n\n\n\n\n\nWe will primarily use Posit Workbench on the servers, where the R version and many R packages are updated regularly\nSee the wiki for details: http://jjnb-wiki-v-00.bio.ri.ccf.org/index.php/Running_R\nLogin in using your Linux credentials at one of the links, for example lri-r07: https://lri-r07.lerner.ccf.org/auth-sign-in\n\n\n\n\nWhen you first open RStudio you will see a number of panes:\n\n\n\nThe layout of the panes can be customized by going to Tools &gt; Global Options &gt; Pane Layout.\nPanes:\n\nText editor - this is where you will type your code, and you will save this file to a project folder for reproducibility\nConsole - this is where the code will be executed\nOther panes will contain a variety of tabs. Some to note include:\n\nEnvironment: where you can see objects and data files that are available in your current session\nFiles: here you should be able to access all folders and files on your home drive\nPlots: this is where plots will disply\nHelp: this is where you will get help files for R functions\nViewer: this is where you would preview any html output like a gt table or Quarto document\n\n\n\n\n\nAlways use a text editor to type code so that it can be saved for reproducibility purposes.\nHow to open a text editor window:\n\nTo open a new text editor window go to: File &gt; New File &gt; R script\nTo save the file go to: File &gt; Save\n\n\n\n\nNavigate to https://lri-r07.lerner.ccf.org/auth-sign-in, log in, and run some example code.\nDemonstrate how to:\n\nCreate a new R script\nAdd some code to it\nRun the code\nView help files\nSave the R script\n\n\nx &lt;- c(1, 2, 3, 4)\nmean(x)\nhist(rnorm(100))\n?rnorm\n\n\n\n\nTo send this to the console to be executed, you can do one of the following:\n\nPlace your cursor next to the line you want to run and hit Ctrl+Enter on your keyboard\nPlace your cursor next to the line you want to run and hit the “Run” button\nHighlight all three lines of code and use one of the previous options\n\n\n\n\nGet help by typing ?fnname where fnname is the name of the function of interest.\n\ne.g. to see the help file for the mean function, type ?mean in the console\n??fnname can be used if you aren’t sure of the exact function name - it will return any keyword matches\n\n\n\n\nAfter we have run all three lines of code, we see the results of our mean computation in the Console pane.\nAnd we see the resulting histogram in the Plots pane.\n\n\n\n\n\n\n\n\n\nCRAN is the primary repository for user-contributed R packages.\nPackages that are on CRAN can be installed using the install.packages() function.\nFor example, we can install the {survival} package from CRAN using:\n\ninstall.packages(\"survival\")\n\nGitHub is a common repository for packages that are in development.\nTo install packages from GitHub, first install the {remotes} package from CRAN:\n\ninstall.packages(\"remotes\")\n\nThen, install the GitHub package of interest using install_github(\"username/repository\"). For example, to install the emo repository from the GitHub username hadley, use:\n\nlibrary(remotes)\ninstall_github(\"hadley/emo\")\n\nOr, avoid a call to the library() function by using the syntax library::function():\n\nremotes::install_github(\"hadley/emo\")\n\nBioconductor is a repository for open source code related to biological data. To install packages from Bioconductor, first install the {BiocManager} package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nThen install, for example, the {GenomicFeatures} package using the install function:\n\nBiocManager::install(\"GenomicFeatures\")\n\nInstallation is the first step. Only needs to be done once.\nLoading is the next step. Must be done every time you open a new R session in which you need to use the package.\nThere are two methods for loading R packages:\n\nA call to library() loads the package for your entire R session.\n\n\nlibrary(survival)\nsurvfit(formula, ...)\n\n\nUsing :: accesses the package only for a single function.\n\n\nsurvival::survfit(formula, ...)\n\n\n\n\nThe most common data format we work with are data from Excel. The example trial dataset is available in the {gtsummary} package in R.\n\nInstall and load the “gtsummary” package\nPreview the trial data by simply typing the name of the dataset:\n\n\ninstall.packages(\"gtsummary\")\n\n\nlibrary(gtsummary)\ntrial\n\n# A tibble: 200 × 8\n   trt      age marker stage grade response death ttdeath\n   &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;    &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;\n 1 Drug A    23  0.16  T1    II           0     0    24  \n 2 Drug B     9  1.11  T2    I            1     0    24  \n 3 Drug A    31  0.277 T1    II           0     0    24  \n 4 Drug A    NA  2.07  T3    III          1     1    17.6\n 5 Drug A    51  2.77  T4    III          1     1    16.4\n 6 Drug B    39  0.613 T4    I            0     1    15.6\n 7 Drug A    37  0.354 T1    II           0     0    24  \n 8 Drug A    32  1.74  T1    I            0     1    18.4\n 9 Drug A    31  0.144 T1    II           0     0    24  \n10 Drug B    34  0.205 T3    I            0     1    10.5\n# ℹ 190 more rows\n\n\nThe help page for the trial data can be accessed by running:\n\n?trial\n\nAnd we see the variables and their definitions:\n\n\n\nI have saved this data file out to Excel so that we can practice loading it in R. Go to the course website to download the file named “trial-Excel.xlsx”. Save it somewhere you can find it again.\n\n\n\nThe most common data format we work with are data from Excel.\nData should be:\n\nOne dataset per file\nA single row of column headers across the top\nSimple column names are better - they will get transformed into variable names by R\nTypically one row per patient/sample is ideal\n\nWe will look at options to:\n\nRead in Excel files with {readxl}\nRead in Excel files by converting to .csv first\nRead in other file formats\n\nFirst, install the {readxl} package from CRAN, then load the newly installed package:\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\nThen use the read_excel() function with the appropriate filepath to read in the data and create an object called “mydf”:\n\nmydf &lt;- read_excel(\n  \"~/MMED/MMED501/data/trial-Excel.xlsx\"\n  )\n\nNote that R treats the \\ as a special character so you either need to use / or \\(\\backslash \\backslash\\) in file paths.\nAlso note that on the Linux server (i.e. on Rstudio Pro via Posit Workbench), the path starts at /home/username, so filepaths relative to your directory can start “~/” followed by the folder where the file is located.\nAlternatively, we can convert the file from .xlsx format to .csv format first, and then read it in.\nAdvantages: removes some of the possible formatting pitfalls associated with Excel files, and you don’t need any special packages to read this format.\n\nOpen the Excel file.\nGo to File &gt; Save As and select “CSV (Comma delimited)” from the “Save as type” drop down and save the file to the same location as “trial-csv.csv”\nUse the read.csv() function with the appropriate file path to read in the data and create an object called “mycsv”\n\n\nmycsv &lt;- \n  read.csv(\n    \"~/MMED/MMED501/data/trial-csv.csv\"\n    ) \n\nNote that this is the approach I always use myself and will form the basis of all of my examples\nMany other file formats exist, and here is a non-comprehensive list of functions for loading some of them:\n\nread.table() is the most generic function and can read many file types\nread.csv() is a special case with fixed defaults for comma-separated files\nread.csv2() is a special case with fixed defaults for comma-separated files that were created in a country where commas are used in place of decimal places\nread.delim() is a special case with fixed defaults for tab-delimited files\nread.delim2() is a special case with fixed defaults for tab-delimited files that were created in a country where commas are used in place of decimal places\n\n\n\n\nUse the assignment operator &lt;- to assign values to objects\n\n&lt;- assigns values on the right, to objects on the left\nKeyboard shortcut “Alt” + “-” will insert the assignment operator\n\n\nx &lt;- 55\nx\n\n[1] 55\n\n\nFunctions are pre-packaged scripts that automate more complicated procedures. They are executed by typing the name of the function followed by round brackets. Inside the round brackets we can provide one or more parameters, or arguments:\n\nx &lt;- 144\nsqrt(x)\n\n[1] 12\n\ny &lt;- 123.225\nround(y)\n\n[1] 123\n\nround(y, digits = 1)\n\n[1] 123.2\n\n\nUse c() to create a vector of values or seq() to create a sequence of values:\n\na &lt;- c(1, 2, 3, 4)\nb &lt;- seq(from = 0, to = 100, by = 10)\nb &lt;- seq(0, 100, 10)\nb &lt;- seq(by = 10, to = 100, from = 0)\n\nNote: when we supply the arguments to a function in the order in which they are listed in the documentation, we do not need to name them. If we name them, we can supply them in any order. The above three assignments to b yield the same results.\nHere are all of the possible arguments to the seq() function:\n\nseq(from = 1, to = 1, by = ((to - from)/(length.out - 1)),\n    length.out = NULL, along.with = NULL, ...)\n\nR is case sensitive:\n\ni.e. age is not the same as Age.\nVariable names with spaces are problematic:\n\nDepending on how you read your data in, R may or may not automatically reformat variable names\nIf you end up with a variable called, e.g. “Patient Age” you would need to reference it in backticks: `Patient Age`\nOne option is to use the clean_names() function from the {janitor} package to convert all variable names to snake case (or alternatives):\n\n\n\ninstall.packages(\"janitor\")\njanitor::clean_names(df)\n\nThe == operator tests equality between two values:\n\n5 == 5\n\n[1] TRUE\n\n5 == 9\n\n[1] FALSE\n\n\nThe first returns TRUE because 5 does in fact equal 5.\nThe second returns FALSE because 5 is not equal to 9.\nWe’ll need this later when we subset data.\nR has three main indexing operators:\n\nDollar sign: $\nDouble brackets: [[ ]]\nSingle brackets: [ ]\n\nTo access specific variables, use the $ operator in the form dataframe$varname, where dataframe is the name of the object to which we assigned our data set, and varname is the name of the variable of interest\nFor example, to calculate the mean of the variable age in the dataframe mycsv:\n\nmean(mycsv$age, na.rm = TRUE)\n\n[1] 47.2381\n\n\nNote that we need to add the argument na.rm = TRUE to remove missing values from the calculation of the mean, otherwise NA will be returned if missing values are present\nAlternatively, use double brackets in the form dataframe[[\"varname\"]]\n\nmean(mycsv[[\"age\"]], na.rm = TRUE)\n\n[1] 47.2381\n\n\nSometimes we may want to create a subset of our data, or access a value based on more than one dimension.\nDatasets typically have two dimensions: columns and rows\nFor dataframe df, let i index the row and j index the column.\nThen we can access any single cell in the dataframe using the syntax:\n\ndf[i, j]\n\nWe can use this concept to create subsets of our data as well.\nWe can create a subset of our data based on the values in a row, for example limiting to patients who were treated with Drug A:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\", ]\nnrow(df_sub)\n\n[1] 98\n\n\nWe see that the new data subset has 98 rows.\nThe & operator signifies “and”.\nSo for example we could subset based on patients who were treated with Drug A AND are over 45 years old:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\" & mycsv$age &gt; 45, ]\nnrow(df_sub)\n\n[1] 55\n\n\nAnd we see that the new data subset has 55 rows.\nThe | operator signifies “or”.\nSo for example we could subset based on patients who were treated with Drug A OR are over 45 years old:\n\ndf_sub &lt;- mycsv[mycsv$trt == \"Drug A\" | mycsv$age &gt; 45, ]\nnrow(df_sub)\n\n[1] 157\n\n\nAnd we see that our new datasubset has 157 rows.\nWe can also create a subset of our data based on columns, for example limiting to trt:\n\ndf_sub &lt;- mycsv[ , c(\"trt\")]\n\nOr we can simultaneously subset based on rows and columns, for example limiting to the trt column among patients with age greater than 45:\n\ndf_sub &lt;- mycsv[mycsv$age &gt; 45, c(\"trt\")]\n\nWe can also subset directly within functions. Suppose we want to calculate the mean of the variable age in the dataframe mycsv, but only among those who were treated with Drug A:\n\nmean(mycsv$age[mycsv$trt == \"Drug A\"], na.rm = TRUE)\n\n[1] 47.01099\n\n\nThis avoids creating additional datasets that may not be needed again.\n\n\n\n\nNext, we’ll review descriptive statistics in R. There are many ways to generate this type of descriptive statistics in R. I will demonstrate one way and if you end up using R a lot you will find what works well for you and the type of data you use.\nWe will be using some functions from the {janitor} package, so make sure you have that installed.\n\ninstall.packages(\"janitor\")\n\n\nlibrary(janitor)\n\nWe can also create very nice tables by also using the {gt} package.\n\ninstall.packages(\"gt\")\n\n\nlibrary(gt)\n\n\n\nFirst, we need to learn about the pipe operator, which we will use to string multiple functions together seamlessly.\nFor example, if we want to take the log transformation of the marker variable and then get the mean, we could nest the two functions as follows:\n\nmean(log(mycsv$marker), na.rm = TRUE)\n\n[1] -0.707878\n\n\nOr we can connect them with the pipe operator:\n\nmycsv$marker |&gt; \n  log() |&gt; \n  mean(na.rm = TRUE)\n\n[1] -0.707878\n\n\nThe left hand side is passed as the first argument to the function on the right hand side.\nYou can use the native pipe operator through the keyboard shortcut ctrl + shift + m.\nThis creates code that is very readable and concise, and easy to comment out various parts if needed.\nWe’ll be using this throughout the R sessions in this course.\n\n\n\nMake sure your variable of interest is in its own column in your dataframe, then use the tabyl() function from {janitor}:\n\nmycsv |&gt; \n  tabyl(trt) |&gt; \n  adorn_pct_formatting()\n\n    trt   n percent\n Drug A  98   49.0%\n Drug B 102   51.0%\n\n\nAnd we get a frequency table for the trt variable, with the percentages formatted using the adorn_pct_formatting() function.\n\n\n\nLet’s create a table with trt on the rows and response on the columns. The most basic table is created as:\n\nmycsv |&gt; \n  tabyl(trt, response)\n\n    trt  0  1 NA_\n Drug A 67 28   3\n Drug B 65 33   4\n\n\nThen we can use the {gt} package and it’s associated features to customize our two-way contingency table. See the {gt} package website for details.\nLet’s label the row variable and column variable, and add a title:\n\nmycsv |&gt; \n  tabyl(trt, response) |&gt; \n  gt(\n    rowname_col = \"trt\" \n  ) |&gt; \n  tab_spanner(          \n    columns = 2:4,\n    label = \"Response?\"\n  ) |&gt; \n  tab_stubhead(         \n    label = \"Treatment\" \n  ) |&gt; \n  tab_header(           \n    title = \"Response according to treatment group\"\n  )\n\n\n\n\n\n\n\nResponse according to treatment group\n\n\nTreatment\nResponse?\n\n\n0\n1\nNA_\n\n\n\n\nDrug A\n67\n28\n3\n\n\nDrug B\n65\n33\n4\n\n\n\n\n\n\n\nAlternatively, we could display percentages in our table:\n\nmycsv |&gt; \n  tabyl(trt, response) |&gt; \n  adorn_percentages(denominator = \"all\") |&gt; \n  gt() |&gt; \n  fmt_percent(\n    columns = -1,\n    decimals = 1\n  ) |&gt; \n  tab_spanner(          \n    columns = 2:4,\n    label = \"Response?\"\n  ) |&gt; \n  tab_stubhead(         \n    label = \"Treatment\" \n  ) |&gt; \n  tab_header(           \n    title = \"Response according to treatment group\"\n  )\n\n\n\n\n\n\n\nResponse according to treatment group\n\n\ntrt\nResponse?\n\n\n0\n1\nNA_\n\n\n\n\nDrug A\n33.5%\n14.0%\n1.5%\n\n\nDrug B\n32.5%\n16.5%\n2.0%\n\n\n\n\n\n\n\n\n\n\nWe will use the summarize() function from the {dplyr} package to compute summary statistics.\n\ninstall.packages(\"dplyr\")\n\n\nlibrary(dplyr)\n\nLet’s compute the mean and standard deviation of age and marker:\n\nmycsv |&gt; \n  summarize(\n    avg_age = mean(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    avg_marker = mean(marker, na.rm = TRUE),\n    sd_marker = sd(marker, na.rm = TRUE)\n  )\n\n  avg_age   sd_age avg_marker sd_marker\n1 47.2381 14.31193  0.9159895 0.8592891\n\n\n\n\n\nAnd we can easily extend this code to generate summary statistics by group by using the group_by() function from the {dplyr} package.\nLet’s get the same summary table for age and marker, but by trt:\n\nmycsv |&gt; \n  group_by(trt) |&gt; \n  summarize(\n    avg_age = mean(age, na.rm = TRUE),\n    sd_age = sd(age, na.rm = TRUE),\n    avg_marker = mean(marker, na.rm = TRUE),\n    sd_marker = sd(marker, na.rm = TRUE)\n  )\n\n# A tibble: 2 × 5\n  trt    avg_age sd_age avg_marker sd_marker\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 Drug A    47.0   14.7      1.02      0.885\n2 Drug B    47.4   14.0      0.821     0.828"
  },
  {
    "objectID": "bio-epi-visualization.html",
    "href": "bio-epi-visualization.html",
    "title": "Biostatistics and Epidemiology - Session 1",
    "section": "",
    "text": "In this session, we will review the basics of R and generating descriptive statistics in R, which were introduced in the Tools course. Then we will learn about data visualization, including creating scatterplots, bar charts, histograms, line charts, and boxplots. We’ll discuss plot customization, faceting, and saving plots. Both univariate and bivariable plotting will be covered.\n\n\nPlotting features are available in base R, but a very popular package for plotting in R is the {ggplot2} package, which will be the focus in this course.\n\n\n\n\ninstall.packages(\"ggplot2\")\n\n\nlibrary(ggplot2)\n\n\n\nScatterplots display the joint distribution of two continuous variables. For example, what if we wanted to see a plot of “marker” by “age”. Put age on the x-axis and marker on the y-axis.\nThe first layer of the plot simply specifies the dataset and the basic aesthetics, in this case just the x-axis and y-axis variables:\n\nggplot(data = mycsv, aes(x = age, y = marker))\n\nThen the scatterplot is created by adding a layer with geom_point() using the + operator:\n\nggplot(data = mycsv, aes(x = age, y = marker)) + \n  geom_point()\n\n\n\n\n\n\n\n\nLook for increasing or decreasing trends, or clusters of points.\n\n\n\nWe can also look at the association between two continuous variables using a line chart.\nThe first layer of the plot specifies the dataset along with the x-axis and y-axis variables:\n\nggplot(data = mycsv, aes(x = age, y = marker))\n\nThen the line chart is created by adding a layer with geom_line() using the + operator.\n\nggplot(data = mycsv, aes(x = age, y = marker)) + \n  geom_line()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in this example, but we would look for increasing or decreasing trends in the line.\n\n\n\nA bar chart can be used to visualize the distribution of a categorical variable.\nThe first layer of the plot specifies the dataset and the x-axis variable:\n\nggplot(data = mycsv, aes(x = grade))\n\nThen the bar chart is created by adding a layer with geom_bar() using the + operator:\n\nggplot(data = mycsv, aes(x = grade)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe height of the bar is the number of observations in that category.\n\n\n\nIt is common to want to describe a continuous variable using a histogram, particularly to examine whether the distribution appears approximately normal.\nLet’s look at a histogram of marker. The first layer of the plot simply specifies the dataset and the basic aesthetics, in this case just the x-axis variable:\n\nggplot(data = mycsv, aes(x = marker))\n\nCreate the histogram by adding a layer with geom_histogram() using the + operator:\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote that we received a message along with our output, regarding the number of bins being used in our histogram. This is prompting you to examine whether the default of 30 bins is appropriate for your given sample size and distribution\nIn this case, we will specify a smaller number of bins for the histogram using the bins = argument to geom_histogram():\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nWe find that the variable “marker” from the trial dataset has a positively skewed distribution.\n\n\n\nBoxplots are another common way of examining continuous variables. The continuous variable is on the y-axis, and there is no variable on the x-axis:\n\nggplot(data = mycsv, aes(y = marker))\n\nAnd the boxplot is created by adding a layer with geom_boxplot() using the + operator:\n\nggplot(data = mycsv, aes(y = marker)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe top and bottom of the box are the 25th and 75th quantiles, the center line is the median, and the whiskers extend to 1.5xIQR.\nWe may also want to make a boxplot according to a categorical variables. Say we are interested in the distribution of marker according to disease grade. We can add an argument to the x-axis of the first layer aesthetics to get separate boxes by grade:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nThere are many customizations available.\nFor example, we can change the x-axis and y-axis labels:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\"\n  )\n\n\n\n\n\n\n\n\nWe can add titles:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  )\n\n\n\n\n\n\n\n\nWe can set axis limits:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  ) + \n  ylim(c(0, 5))\n\n\n\n\n\n\n\n\nAnd we can change the style of the plot using theme elements:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  ) + \n  ylim(c(0, 5)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nThere are many other customizations available, see the ggplot2 website for details and for a helpful cheatsheet of plotting options.\n\n\n\nWhat if it was of interest to see the distribution of marker according to disease grade?\nAdd a layer to our histogram using facet_grid() to get panels for each level of the disease grade variable “grade”.\nNote that by default the y-axis and x-axis limits are fixed across all plots, so we can directly compare the distributions. It is possible to control this with the scales argument to facet_grid(), see ?facet_grid for details.\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram(bins = 10) +\n  facet_grid(cols = vars(grade))\n\n\n\n\n\n\n\n\n\n\n\nWe will often want to save plots to an external file to insert into a later document. There are several options:\n\nUse the interactive plot window to export your created plot.\nUse code to save your plot to an external location. There is a function called ggsave() specifically for saving results of ggplot(). By default it will save the last created plot, or you can save your plot to an object and specify it directly using the plot argument to ggsave(). You will specify the file format by including an extension, here “.png” on your filename. ]\n\n\n\nCreate your plot. Click on “Export” then select the option… 1. “Copy to Clipboard”. This will open a pop-up window where you could rescale the plot, if desired, and then click “Copy Plot” to copy the plot to your clipboard so that you can paste it into any external document. 2. “Save as Image…”. Here you can change the directory location where you want to save the plot to the same location where you have your code and data files for this class saved so far. You can select from a variety of file formats. Save it as “my-histogram” in PNG format. You can then insert this file into other documents as needed. 3. “Save as PDF…” and follow the same instructions as in B to save a .pdf version of your image.\n\n\n\n\np &lt;- ggplot(data = mycsv, aes(y = marker)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(grade))\n    \nggsave(filename = \"H:/MMED/MMED501/plots/my-boxplot.png\", plot = p)\n\nAdjust the width and height using the width and/or height arguments to the ggsave() function"
  },
  {
    "objectID": "bio-epi-visualization.html#visualization",
    "href": "bio-epi-visualization.html#visualization",
    "title": "Biostatistics and Epidemiology - Session 1",
    "section": "",
    "text": "Plotting features are available in base R, but a very popular package for plotting in R is the {ggplot2} package, which will be the focus in this course.\n\n\n\n\ninstall.packages(\"ggplot2\")\n\n\nlibrary(ggplot2)\n\n\n\nScatterplots display the joint distribution of two continuous variables. For example, what if we wanted to see a plot of “marker” by “age”. Put age on the x-axis and marker on the y-axis.\nThe first layer of the plot simply specifies the dataset and the basic aesthetics, in this case just the x-axis and y-axis variables:\n\nggplot(data = mycsv, aes(x = age, y = marker))\n\nThen the scatterplot is created by adding a layer with geom_point() using the + operator:\n\nggplot(data = mycsv, aes(x = age, y = marker)) + \n  geom_point()\n\n\n\n\n\n\n\n\nLook for increasing or decreasing trends, or clusters of points.\n\n\n\nWe can also look at the association between two continuous variables using a line chart.\nThe first layer of the plot specifies the dataset along with the x-axis and y-axis variables:\n\nggplot(data = mycsv, aes(x = age, y = marker))\n\nThen the line chart is created by adding a layer with geom_line() using the + operator.\n\nggplot(data = mycsv, aes(x = age, y = marker)) + \n  geom_line()\n\n\n\n\n\n\n\n\nThere is no discernible pattern in this example, but we would look for increasing or decreasing trends in the line.\n\n\n\nA bar chart can be used to visualize the distribution of a categorical variable.\nThe first layer of the plot specifies the dataset and the x-axis variable:\n\nggplot(data = mycsv, aes(x = grade))\n\nThen the bar chart is created by adding a layer with geom_bar() using the + operator:\n\nggplot(data = mycsv, aes(x = grade)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe height of the bar is the number of observations in that category.\n\n\n\nIt is common to want to describe a continuous variable using a histogram, particularly to examine whether the distribution appears approximately normal.\nLet’s look at a histogram of marker. The first layer of the plot simply specifies the dataset and the basic aesthetics, in this case just the x-axis variable:\n\nggplot(data = mycsv, aes(x = marker))\n\nCreate the histogram by adding a layer with geom_histogram() using the + operator:\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote that we received a message along with our output, regarding the number of bins being used in our histogram. This is prompting you to examine whether the default of 30 bins is appropriate for your given sample size and distribution\nIn this case, we will specify a smaller number of bins for the histogram using the bins = argument to geom_histogram():\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nWe find that the variable “marker” from the trial dataset has a positively skewed distribution.\n\n\n\nBoxplots are another common way of examining continuous variables. The continuous variable is on the y-axis, and there is no variable on the x-axis:\n\nggplot(data = mycsv, aes(y = marker))\n\nAnd the boxplot is created by adding a layer with geom_boxplot() using the + operator:\n\nggplot(data = mycsv, aes(y = marker)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\nThe top and bottom of the box are the 25th and 75th quantiles, the center line is the median, and the whiskers extend to 1.5xIQR.\nWe may also want to make a boxplot according to a categorical variables. Say we are interested in the distribution of marker according to disease grade. We can add an argument to the x-axis of the first layer aesthetics to get separate boxes by grade:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nThere are many customizations available.\nFor example, we can change the x-axis and y-axis labels:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\"\n  )\n\n\n\n\n\n\n\n\nWe can add titles:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  )\n\n\n\n\n\n\n\n\nWe can set axis limits:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  ) + \n  ylim(c(0, 5))\n\n\n\n\n\n\n\n\nAnd we can change the style of the plot using theme elements:\n\nggplot(data = mycsv, aes(y = marker, x = grade)) + \n  geom_boxplot() + \n  labs(\n    x = \"Disease grade\",\n    y = \"Marker level (ng/mL)\",\n    title = \"Boxplot of marker level by disease grade\"\n  ) + \n  ylim(c(0, 5)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nThere are many other customizations available, see the ggplot2 website for details and for a helpful cheatsheet of plotting options.\n\n\n\nWhat if it was of interest to see the distribution of marker according to disease grade?\nAdd a layer to our histogram using facet_grid() to get panels for each level of the disease grade variable “grade”.\nNote that by default the y-axis and x-axis limits are fixed across all plots, so we can directly compare the distributions. It is possible to control this with the scales argument to facet_grid(), see ?facet_grid for details.\n\nggplot(data = mycsv, aes(x = marker)) + \n  geom_histogram(bins = 10) +\n  facet_grid(cols = vars(grade))\n\n\n\n\n\n\n\n\n\n\n\nWe will often want to save plots to an external file to insert into a later document. There are several options:\n\nUse the interactive plot window to export your created plot.\nUse code to save your plot to an external location. There is a function called ggsave() specifically for saving results of ggplot(). By default it will save the last created plot, or you can save your plot to an object and specify it directly using the plot argument to ggsave(). You will specify the file format by including an extension, here “.png” on your filename. ]\n\n\n\nCreate your plot. Click on “Export” then select the option… 1. “Copy to Clipboard”. This will open a pop-up window where you could rescale the plot, if desired, and then click “Copy Plot” to copy the plot to your clipboard so that you can paste it into any external document. 2. “Save as Image…”. Here you can change the directory location where you want to save the plot to the same location where you have your code and data files for this class saved so far. You can select from a variety of file formats. Save it as “my-histogram” in PNG format. You can then insert this file into other documents as needed. 3. “Save as PDF…” and follow the same instructions as in B to save a .pdf version of your image.\n\n\n\n\np &lt;- ggplot(data = mycsv, aes(y = marker)) + \n  geom_boxplot() + \n  facet_grid(cols = vars(grade))\n    \nggsave(filename = \"H:/MMED/MMED501/plots/my-boxplot.png\", plot = p)\n\nAdjust the width and height using the width and/or height arguments to the ggsave() function"
  },
  {
    "objectID": "tools-install-packages.html",
    "href": "tools-install-packages.html",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "In the next part of Session 1, we will learn how to install and load R packages.\n\n\n\n\n\n\n\n\nR package\n\n\n\nAn R package is a collection of code, data, documentation, and tests.\nR packages are developed by the community.\nR packages add functionality to what comes in base R.\nTo use R packages, there are two steps:\n\nInstall\nLoad\n\n\n\n\n\n\nWe will cover the second step first, because it is the same for all methods of installing packages and we will reference this in covering some of the methods of installation.\nInstallation is the first step. Only needs to be done once. (Though needs to be repeated if you get a new version of R)\nLoading is the next step. Must be done every time you open a new R session in which you need to use the package.\nThere are two methods for loading R packages:\n\nA call to library() loads the package for your entire R session.\n\n\nlibrary(survival)\nsurvfit(formula, ...)\n\n\nUsing :: accesses the package only for a single function.\n\n\nsurvival::survfit(formula, ...)\n\n\n\n\nThere are three main repositories for R packages: CRAN, GitHub, and Bioconductor\n\n\nCRAN is the primary repository for user-contributed R packages.\nPackages that are on CRAN can be installed using the install.packages() function.\nFor example, we can install the {survival} package from CRAN using:\n\ninstall.packages(\"survival\")\n\nThis package has comprehensive functionality for survival analyses.\n\n\n\nGitHub is a common repository for packages that are still in development, or have not developed thoroughly enough to be accepted onto CRAN.\nTo install packages from GitHub, first install the {remotes} package from CRAN:\n\ninstall.packages(\"remotes\")\n\nNext load the {remotes} package using a call to library():\n\nlibrary(remotes)\n\nThen install the GitHub package of interest using install_github(\"username/repository\"), where “username” is the name of the GitHub user and “repository” is the name of the repository under the user’s account. For example, to install the emo repository from the GitHub user with username hadley, use:\n\ninstall_github(\"hadley/emo\")\n\nOr, avoid a call to the library() function by using the syntax library::function():\n\nremotes::install_github(\"hadley/emo\")\n\nThis package will let you put emojis in your documents 😄\n\n\n\nBioconductor is a repository for open source code related to biological data. To install packages from Bioconductor, first install the {BiocManager} package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nThen install, for example, the {ggtree} package with the install function, using either the library::function() syntax:\n\nBiocManager::install(\"ggtree\")\n\nOr using a call to library() followed by a separate call to install():\n\nlibrary(BiocManager)\ninstall(\"ggtree\")\n\nThis package is designed for visualization and annotation of phylogentic trees.\n\n\n\n\n\n\nTip\n\n\n\nWhen using Posit Workbench, many commonly used packages have already been installed for the current version of R by the system administrator.\nAlways start by trying to load a package of interest first to see if it is available.\n\n\n\n\n\n\n\n\nTip\n\n\n\nR scripts are used for programming and saved with other project-related documents for reproducibility purposes.\nIt is important to include statements to load the packages used in any analysis in the R script.\nHowever, I do not think it is necessary to include statements to install the packages in the R script. I personally type these statements directly into the console since they do not need to be run every time and therefore are not part of the reproducibility pipeline.\n\n\n\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 2\n\n\n\n\n\n\nOpen the R script we created earlier named “session1-exercies.R”\nLoad packages we will need in the rest of today’s session, including: janitor, here, readr, readxl\n\nNote that these packages are already available on the Posit Workbench servers. If you are using RStudio on your personal computer instead, you will need to install them if you have not done so already.\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 2 Solution\n\n\n\n\n\nIf using Posit Workbench where the packages are already installed:\n\nlibrary(janitor)\nlibrary(here)\nlibrary(readr)\nlibrary(readxl)\n\nIf using a personal computer where you have not previousling installed these packages:\n\ninstall.packages(\"janitor\")\ninstall.packages(\"here\")\ninstall.packages(\"readr\")\ninstall.packages(\"readxl\")\nlibrary(janitor)\nlibrary(here)\nlibrary(readr)\nlibrary(readxl)"
  },
  {
    "objectID": "tools-install-packages.html#what-is-an-r-package",
    "href": "tools-install-packages.html#what-is-an-r-package",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "R package\n\n\n\nAn R package is a collection of code, data, documentation, and tests.\nR packages are developed by the community.\nR packages add functionality to what comes in base R.\nTo use R packages, there are two steps:\n\nInstall\nLoad"
  },
  {
    "objectID": "tools-install-packages.html#loading-r-packages",
    "href": "tools-install-packages.html#loading-r-packages",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "We will cover the second step first, because it is the same for all methods of installing packages and we will reference this in covering some of the methods of installation.\nInstallation is the first step. Only needs to be done once. (Though needs to be repeated if you get a new version of R)\nLoading is the next step. Must be done every time you open a new R session in which you need to use the package.\nThere are two methods for loading R packages:\n\nA call to library() loads the package for your entire R session.\n\n\nlibrary(survival)\nsurvfit(formula, ...)\n\n\nUsing :: accesses the package only for a single function.\n\n\nsurvival::survfit(formula, ...)"
  },
  {
    "objectID": "tools-install-packages.html#installing-r-packages",
    "href": "tools-install-packages.html#installing-r-packages",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "There are three main repositories for R packages: CRAN, GitHub, and Bioconductor\n\n\nCRAN is the primary repository for user-contributed R packages.\nPackages that are on CRAN can be installed using the install.packages() function.\nFor example, we can install the {survival} package from CRAN using:\n\ninstall.packages(\"survival\")\n\nThis package has comprehensive functionality for survival analyses.\n\n\n\nGitHub is a common repository for packages that are still in development, or have not developed thoroughly enough to be accepted onto CRAN.\nTo install packages from GitHub, first install the {remotes} package from CRAN:\n\ninstall.packages(\"remotes\")\n\nNext load the {remotes} package using a call to library():\n\nlibrary(remotes)\n\nThen install the GitHub package of interest using install_github(\"username/repository\"), where “username” is the name of the GitHub user and “repository” is the name of the repository under the user’s account. For example, to install the emo repository from the GitHub user with username hadley, use:\n\ninstall_github(\"hadley/emo\")\n\nOr, avoid a call to the library() function by using the syntax library::function():\n\nremotes::install_github(\"hadley/emo\")\n\nThis package will let you put emojis in your documents 😄\n\n\n\nBioconductor is a repository for open source code related to biological data. To install packages from Bioconductor, first install the {BiocManager} package from CRAN:\n\ninstall.packages(\"BiocManager\")\n\nThen install, for example, the {ggtree} package with the install function, using either the library::function() syntax:\n\nBiocManager::install(\"ggtree\")\n\nOr using a call to library() followed by a separate call to install():\n\nlibrary(BiocManager)\ninstall(\"ggtree\")\n\nThis package is designed for visualization and annotation of phylogentic trees.\n\n\n\n\n\n\nTip\n\n\n\nWhen using Posit Workbench, many commonly used packages have already been installed for the current version of R by the system administrator.\nAlways start by trying to load a package of interest first to see if it is available.\n\n\n\n\n\n\n\n\nTip\n\n\n\nR scripts are used for programming and saved with other project-related documents for reproducibility purposes.\nIt is important to include statements to load the packages used in any analysis in the R script.\nHowever, I do not think it is necessary to include statements to install the packages in the R script. I personally type these statements directly into the console since they do not need to be run every time and therefore are not part of the reproducibility pipeline."
  },
  {
    "objectID": "tools-install-packages.html#exercises",
    "href": "tools-install-packages.html#exercises",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "Session 1, Exercise 2\n\n\n\n\n\n\nOpen the R script we created earlier named “session1-exercies.R”\nLoad packages we will need in the rest of today’s session, including: janitor, here, readr, readxl\n\nNote that these packages are already available on the Posit Workbench servers. If you are using RStudio on your personal computer instead, you will need to install them if you have not done so already.\n\n\n\n\n\n\n\n\n\nSession 1, Exercise 2 Solution\n\n\n\n\n\nIf using Posit Workbench where the packages are already installed:\n\nlibrary(janitor)\nlibrary(here)\nlibrary(readr)\nlibrary(readxl)\n\nIf using a personal computer where you have not previousling installed these packages:\n\ninstall.packages(\"janitor\")\ninstall.packages(\"here\")\ninstall.packages(\"readr\")\ninstall.packages(\"readxl\")\nlibrary(janitor)\nlibrary(here)\nlibrary(readr)\nlibrary(readxl)"
  },
  {
    "objectID": "tools-loading-data.html",
    "href": "tools-loading-data.html",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "In this part of Session 1, we will learn how to load external data into R.\nMany R packages come with data bundled into them. These datasets are available for use as soon as the package is loaded.\nHowever, we often need to load external data that we’ve generated in our lab or in a clinical study, a situation addressed in this lesson.\n\n\n\n\n\n\nFormatting data for use in R\n\n\n\nIf you are creating your own datasets for future analysis in R, or advising someone else about how best to do so, here are some tips:\n\nOne row per patient/subject\nColumn with a unique identifier for each subject (i.e. a patient ID)\n\nOR, if you have longitudinal or other repeated measures data, have another column identifying the repeat instance and then there can be multiple rows per patient for each repeat instance\n\nOne row of column labels (i.e. avoid a second row of headers where some cells are merged, etc)\nOne measurement with one column name in each column (i.e. avoid separating multiple pieces of data by commas, semicolons, etc within a cell)\nSimple variable names for each column - avoid long names and special characters\n\n\n\n\n\nThe dataset used in this course is stored in this folder\n\n\n\nThe most common data format we work with are data from Excel.\nWe will look at options to:\n\nRead in Excel files with {readxl}\nRead in Excel files by converting to .csv first\nRead in other file formats\n\nFirst, install the {readxl} package from CRAN, then load the newly installed package:\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\nThen use the read_excel() function with the appropriate filepath to read in the data and create an object called “mydf”:\n\nmydf &lt;- read_excel(\n  \"~/MMED/MMED501/data/trial-Excel.xlsx\"\n  )\n\nNote that R treats the \\ as a special character so you either need to use / or \\(\\backslash \\backslash\\) in file paths.\nAlso note that on the Linux server (i.e. on Rstudio Pro via Posit Workbench), the path starts at /home/username, so filepaths relative to your directory can start “~/” followed by the folder where the file is located.\nAlternatively, we can convert the file from .xlsx format to .csv format first, and then read it in.\nAdvantages: removes some of the possible formatting pitfalls associated with Excel files, and you don’t need any special packages to read this format.\n\nOpen the Excel file.\nGo to File &gt; Save As and select “CSV (Comma delimited)” from the “Save as type” drop down and save the file to the same location as “trial-csv.csv”\nUse the read.csv() function with the appropriate file path to read in the data and create an object called “mycsv”\n\n\nmycsv &lt;- \n  read.csv(\n    \"~/MMED/MMED501/data/trial-csv.csv\"\n    ) \n\nNote that this is the approach I always use myself and will form the basis of all of my examples\nMany other file formats exist, and here is a non-comprehensive list of functions for loading some of them:\n\nread.table() is the most generic function and can read many file types\nread.csv() is a special case with fixed defaults for comma-separated files\nread.csv2() is a special case with fixed defaults for comma-separated files that were created in a country where commas are used in place of decimal places\nread.delim() is a special case with fixed defaults for tab-delimited files\nread.delim2() is a special case with fixed defaults for tab-delimited files that were created in a country where commas are used in place of decimal places"
  },
  {
    "objectID": "tools-loading-data.html#download-the-data-for-use-in-this-class",
    "href": "tools-loading-data.html#download-the-data-for-use-in-this-class",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "The dataset used in this course is stored in this folder"
  },
  {
    "objectID": "tools-loading-data.html#loading-data",
    "href": "tools-loading-data.html#loading-data",
    "title": "Tools course - R Session 1",
    "section": "",
    "text": "The most common data format we work with are data from Excel.\nWe will look at options to:\n\nRead in Excel files with {readxl}\nRead in Excel files by converting to .csv first\nRead in other file formats\n\nFirst, install the {readxl} package from CRAN, then load the newly installed package:\n\ninstall.packages(\"readxl\")\nlibrary(readxl)\n\nThen use the read_excel() function with the appropriate filepath to read in the data and create an object called “mydf”:\n\nmydf &lt;- read_excel(\n  \"~/MMED/MMED501/data/trial-Excel.xlsx\"\n  )\n\nNote that R treats the \\ as a special character so you either need to use / or \\(\\backslash \\backslash\\) in file paths.\nAlso note that on the Linux server (i.e. on Rstudio Pro via Posit Workbench), the path starts at /home/username, so filepaths relative to your directory can start “~/” followed by the folder where the file is located.\nAlternatively, we can convert the file from .xlsx format to .csv format first, and then read it in.\nAdvantages: removes some of the possible formatting pitfalls associated with Excel files, and you don’t need any special packages to read this format.\n\nOpen the Excel file.\nGo to File &gt; Save As and select “CSV (Comma delimited)” from the “Save as type” drop down and save the file to the same location as “trial-csv.csv”\nUse the read.csv() function with the appropriate file path to read in the data and create an object called “mycsv”\n\n\nmycsv &lt;- \n  read.csv(\n    \"~/MMED/MMED501/data/trial-csv.csv\"\n    ) \n\nNote that this is the approach I always use myself and will form the basis of all of my examples\nMany other file formats exist, and here is a non-comprehensive list of functions for loading some of them:\n\nread.table() is the most generic function and can read many file types\nread.csv() is a special case with fixed defaults for comma-separated files\nread.csv2() is a special case with fixed defaults for comma-separated files that were created in a country where commas are used in place of decimal places\nread.delim() is a special case with fixed defaults for tab-delimited files\nread.delim2() is a special case with fixed defaults for tab-delimited files that were created in a country where commas are used in place of decimal places"
  }
]